services:
  proxy:
    image: proxy
    build:
      context: ./proxy
      dockerfile: Dockerfile
    container_name: proxy
    restart: unless-stopped
    ports:
      - "8090:8090"
    depends_on:
      grafana:
        condition: service_healthy
      prometheus:
        condition: service_healthy
      node-exporter:
        condition: service_healthy
      alertmanager:
        condition: service_healthy
      auth-service:
        condition: service_healthy
      history-service:
        condition: service_healthy
      tournament-service:
        condition: service_healthy
      #ELK
      # kibana:
      #   condition: service_healthy
      # es01:
      #   condition: service_healthy
    volumes:
    #TODO check if logs works bind mount or they need to be in a local volume
      - media:/app/media:rw 
      - ./proxy/logs:/var/log/nginx
    healthcheck:
      test: ["CMD-SHELL", "curl -f -k https://localhost:8090 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - internal

  auth-service:
    image: auth-service
    container_name: auth-service
    build:
      context: ./backend/auth_service
      dockerfile: Dockerfile
    command: bash -c "/app/start.sh"
    env_file:
      - .env
    restart: unless-stopped
    volumes:
      - media:/app/media:rw
    depends_on:
      auth_db:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8001/watchman/?skip=watchman.checks.storage || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - internal

  auth_db:
    image: postgres:15
    container_name: auth_db
    env_file: ".env"
    environment:
     - POSTGRES_USER=${POSTGRES_USER}
     - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
     - POSTGRES_DB=${POSTGRES_AUTH_DB}
     - POSTGRES_HOST=${POSTGRES_AUTH_HOST}
    restart: always
    volumes:
      - postgres_auth:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - internal


  tournament-service:
    image: tournament-service
    container_name: tournament-service
    build:
      context: ./backend/tournament-service
      dockerfile: Dockerfile
    command: >
      bash -c "python manage.py makemigrations &&
               python manage.py migrate &&
               gunicorn tournament.wsgi:application --bind 0.0.0.0:8003 --workers=3"
    env_file:
      - .env
    restart: unless-stopped
    depends_on:
      tournament_db:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8003/watchman/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - internal

  tournament_db:
    image: postgres:15
    container_name: tournament_db
    env_file: ".env"
    environment:
     - POSTGRES_USER=${POSTGRES_USER}
     - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
     - POSTGRES_DB=${POSTGRES_TOURNAMENT_DB}
     - POSTGRES_HOST=${POSTGRES_TOURNAMENT_HOST}
    restart: always
    volumes:
      - postgres_tournament:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - internal

  history-service:
    image: history-service
    container_name: history-service
    build:
      context: ./backend/history
      dockerfile: Dockerfile
    command: >
      bash -c "python manage.py makemigrations &&
               python manage.py migrate &&
               gunicorn history_service.wsgi:application --bind 0.0.0.0:8002 --workers=3"
    env_file:
      - .env
    restart: unless-stopped
    depends_on:
      history_db:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8002/watchman/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - internal

  history_db:
    image: postgres:15
    container_name: history_db
    env_file: ".env"
    environment:
     - POSTGRES_USER=${POSTGRES_USER}
     - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
     - POSTGRES_DB=${POSTGRES_HISTORY_DB}
     - POSTGRES_HOST=${POSTGRES_HISTORY_HOST}
    restart: always
    volumes:
      - postgres_history:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - internal

  #monitoring
  grafana:
    image: grafana
    build:
      context: ./monitoring/grafana
      dockerfile: Dockerfile
    container_name: grafana
    env_file: ".env"
    restart: unless-stopped
    environment:
      - GF_SERVER_ROOT_URL=https://localhost:8090/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH="true"
    depends_on:
      prometheus:
        condition: service_healthy
      grafana_db:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - internal

  grafana_db:
    image: postgres:15
    container_name: grafana_db
    env_file: ".env"
    environment:
     - POSTGRES_USER=${POSTGRES_USER}
     - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
     - POSTGRES_DB=${POSTGRES_G_DB}
     - POSTGRES_HOST=${POSTGRES_G_HOST}
    restart: always
    volumes:
      - postgres_grafana:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - internal

  
  prometheus:
    image: prometheus
    build:
      context: ./monitoring/prometheus
      dockerfile: Dockerfile
    container_name: prometheus
    restart: unless-stopped
    volumes:
      - prometheus_data:/prometheus/data
    depends_on:
      auth-service:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:9090/-/healthy || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - internal

  node-exporter:
    image: node-exporter
    build:
      context: ./monitoring/node-exporter
      dockerfile: Dockerfile
    container_name: node-exporter
    restart: unless-stopped
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget --spider http://localhost:9100 || exit 1"]
      interval: 30s
      retries: 3
      start_period: 10s
      timeout: 10s
    networks: 
      - internal

  alertmanager:
    image: alertmanager
    build:
      context: ./monitoring/alertmanager
      dockerfile: Dockerfile
    container_name: alertmanager
    env_file: ".env"
    restart: unless-stopped
    depends_on:
     - prometheus
    volumes:
      - alertmanager_data:/data
    healthcheck:
      test: ["CMD-SHELL", "wget --spider http://localhost:9093 || exit 1"]
      interval: 30s
      retries: 3
      start_period: 10s
      timeout: 10s
    networks:
      - internal

  nginx-prometheus-exporter:
    image: nginx/nginx-prometheus-exporter:1.4
    container_name: nginx-prometheus-exporter
    restart: unless-stopped
    depends_on:
      - proxy
    command: [
      "--nginx.scrape-uri", "http://proxy:8080/stub_status",
    ]
    networks:
      - internal

  postgres-exporter-grafana:
    image: prometheuscommunity/postgres-exporter:v0.16.0
    container_name: postgres-exporter-grafana
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@grafana_db:5432/grafana_db?sslmode=disable"
    restart: unless-stopped
    command:
      - '--web.listen-address=:9188'
    depends_on:
      prometheus:
        condition: service_healthy
      grafana_db:
        condition: service_healthy
    networks:
      - internal

  postgres-exporter-auth-db:
    image: prometheuscommunity/postgres-exporter:v0.16.0
    container_name: postgres-exporter-auth-db
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@auth_db:5432/auth_db?sslmode=disable"
    restart: unless-stopped
    depends_on:
      prometheus:
        condition: service_healthy
      auth_db:
        condition: service_healthy
    networks:
      - internal

  postgres-exporter-tournament-db:
    image: prometheuscommunity/postgres-exporter:v0.16.0
    container_name: postgres-exporter-tournament-db
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@tournament_db:5432/tournament_db?sslmode=disable"
    restart: unless-stopped
    command:
      - '--web.listen-address=:9189'
    depends_on:
      prometheus:
        condition: service_healthy
      tournament_db:
        condition: service_healthy
    networks:
      - internal

  postgres-exporter-history-db:
    image: prometheuscommunity/postgres-exporter:v0.16.0
    container_name: postgres-exporter-history-db
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@history_db:5432/history_db?sslmode=disable"
    restart: unless-stopped
    command:
      - '--web.listen-address=:9190'
    depends_on:
      prometheus:
        condition: service_healthy
      history_db:
        condition: service_healthy
    networks:
      - internal

  adminer:
    image: adminer
    container_name: adminer
    restart: unless-stopped
    networks:
      - internal

#ELK STACK
  # setup:
  #   image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
  #   container_name: setup
  #   environment:
  #     - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
  #     - KIBANA_PASSWORD=${KIBANA_PASSWORD}
  #   command: /usr/share/elasticsearch/setup.sh
  #   volumes:
  #     - certs:/usr/share/elasticsearch/config/certs
  #     - ./ELK/setup.sh:/usr/share/elasticsearch/setup.sh:ro
  #     - ./ELK/elasticsearch/config/elastic:/usr/share/elasticsearch/config/elastic
  #   user: "0"
  #   healthcheck:
  #     test: ["CMD-SHELL", "[ -f config/certs/es01/es01.crt ]"]
  #     interval: 1s
  #     timeout: 5s
  #     retries: 120
  #   networks:
  #     - internal

  # es01:
  #   image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
  #   container_name: es01
  #   labels:
  #     co.elastic.logs/module: elasticsearch
  #   environment:
  #     - network.host=0.0.0.0
  #     - ES_JAVA_OPTS=-Xmx512m -Xms512m
  #     - node.name=es01
  #     - cluster.name=${CLUSTER_NAME}
  #     - discovery.type= single-node #single node version
  #     # - cluster.initial_master_nodes=es01,es02,es03
  #     # - discovery.seed_hosts=es01,es02,es03
  #     - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
  #     - bootstrap.memory_lock=true
  #     - xpack.security.enabled=true
  #     - xpack.security.http.ssl.enabled=true
  #     - xpack.security.http.ssl.key=certs/es01/es01.key
  #     - xpack.security.http.ssl.certificate=certs/es01/es01.crt
  #     - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
  #     - xpack.security.transport.ssl.enabled=true
  #     - xpack.security.transport.ssl.key=certs/es01/es01.key
  #     - xpack.security.transport.ssl.certificate=certs/es01/es01.crt
  #     - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
  #     - xpack.security.transport.ssl.verification_mode=certificate
  #     - xpack.license.self_generated.type=${LICENSE}
  #   volumes:
  #     - certs:/usr/share/elasticsearch/config/certs
  #     - es_data01:/usr/share/elasticsearch/data
  #   depends_on:
  #     setup:
  #       condition: service_healthy
  #   mem_limit: 2g
  #   ulimits:
  #     memlock:
  #       soft: -1
  #       hard: -1
  #   healthcheck:
  #     test:
  #       [
  #         "CMD-SHELL",
  #         "curl -s -u elastic:${ELASTIC_PASSWORD} --cacert config/certs/ca/ca.crt https://localhost:9200/_cat/health | grep -E 'green'"
  #         # "curl -s -u elastic:${ELASTIC_PASSWORD} --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -E 'my-cluster'",
  #       ]
  #     interval: 10s
  #     timeout: 10s
  #     retries: 120
  #     # start_period: 60s
  #   networks:
  #     - internal

  # kibana:
  #   image: docker.elastic.co/kibana/kibana:${STACK_VERSION}
  #   container_name: kibana
  #   labels:
  #     co.elastic.logs/module: kibana
  #   environment:
  #     - SERVERNAME=kibana
  #     - SERVER_PUBLICBASEURL=https://localhost/kibana
  #     - SERVER_REWRITEBASEPATH=true
  #     - ELASTICSEARCH_HOSTS=https://es01:9200
  #     - ELASTICSEARCH_USERNAME=kibana_system
  #     - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
  #     - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt
  #     - XPACK_SECURITY_ENCRYPTIONKEY=${ENCRYPTION_KEY}
  #     - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=${ENCRYPTION_KEY}
  #     - XPACK_REPORTING_ENCRYPTIONKEY=${ENCRYPTION_KEY}
  #     - XPACK_REPORTING_ROLES_ENABLED=false
  #     - XPACK_REPORTING_KIBANASERVER_HOSTNAME=localhost
  #   volumes:
  #     - certs:/usr/share/kibana/config/certs
  #     - ./ELK/kibana/kibana.yml:/usr/share/kibana/config/kibana.yml
  #     - kibana_data:/usr/share/kibana/data
  #   depends_on:
  #     es01:
  #       condition: service_healthy
  #   healthcheck:
  #     test:
  #       [
  #         "CMD-SHELL",
  #         "curl -f -k https://localhost:5601/kibana/api/status || exit 1"
  #       ]
  #     interval: 10s
  #     timeout: 10s
  #     retries: 120
  #   mem_limit: 1g
  #   networks:
  #     - internal

  # logstash:
  #   image: docker.elastic.co/logstash/logstash:${STACK_VERSION}
  #   container_name: logstash
  #   labels:
  #     co.elastic.logs/module: logstash
  #   environment:
  #     - LS_JAVA_OPTS=-Xmx512m -Xms512m
  #     - NODE_NAME="logstash"
  #     - xpack.monitoring.enabled=false
  #     - ELASTIC_USER=elastic
  #     - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
  #     - ELASTIC_HOSTS=https://es01:9200
  #   command: logstash -f /usr/share/logstash/pipeline/logstash.conf
  #   user: root
  #   volumes:
  #     - ./ELK/logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro
  #     - ./ELK/logstash/logstash.yml:/usr/share/logstash/config/logstash.yml
  #     - ./proxy/logs:/var/log/nginx  # Mount Nginx logs to Logstash
  #     - logstash_data:/usr/share/logstash/data
  #     - certs:/usr/share/logstash/certs
  #     # - proxy_logs:/var/log/nginx #mount nginx logs
  #   depends_on:
  #     es01:
  #       condition: service_healthy
  #     kibana:
  #       condition: service_healthy
  #   mem_limit: 1g
  #   networks:
  #     - internal

networks:
  internal:
    driver: bridge
  # monitoring:
  #   driver: bridge

volumes:
#TODO check if logs works with local volume
  # proxy_logs: 
  postgres_auth:
  postgres_grafana:
  prometheus_data:
  alertmanager_data:
  postgres_tournament:
  postgres_history:
  #ELK
  certs:
  es_data01:
  kibana_data:
  logstash_data:
  media: